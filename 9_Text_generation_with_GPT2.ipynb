{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy2nSHtpvVnn",
        "outputId": "cf3cd4b4-ff5b-4b22-8119-41274f862b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TifY3JRuw7A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset link: https://www.kaggle.com/datasets/amaanmansuri/humor-detection <br>This dataset has around 20k texts. Half of them are jokes. For generation I'll use only them\n"
      ],
      "metadata": {
        "id": "F8Q3J8OpLHBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jokes = pd.read_csv('Humour.csv')\n",
        "jokes_df = jokes[jokes['humor']==True][:10000]\n",
        "jokes_df = jokes_df.drop('humor', axis=1)"
      ],
      "metadata": {
        "id": "0k59VQzYvnh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = jokes_df.sample(n = 30)\n",
        "jokes_df = jokes_df.loc[~jokes_df.index.isin(test_set.index)]\n",
        "\n",
        "#Reset the indexes\n",
        "test_set = test_set.reset_index()\n",
        "test_set = test_set.drop('index', axis=1)\n",
        "jokes_df = jokes_df.reset_index()\n",
        "jokes_df = jokes_df.drop('index', axis=1)\n",
        "\n",
        "#For the test set only, keep last 5 words in a new column, then remove them from original column\n",
        "test_set['True end'] = test_set['text'].str.split().str[5:].apply(' '.join)\n",
        "test_set['text'] = test_set['text'].str.split().str[:5].apply(' '.join)"
      ],
      "metadata": {
        "id": "IeC7W0GD2Y0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jokes_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "JahO-oVb4MYd",
        "outputId": "f90eb7ac-4be0-446b-fd4e-d2115214881c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text\n",
              "0     What do you call a turtle without its shell? d...\n",
              "1     What is a pokemon master's favorite kind of pa...\n",
              "2     Why do native americans hate it when it rains ...\n",
              "3         My family tree is a cactus, we're all pricks.\n",
              "4     How are music and candy similar? we throw away...\n",
              "...                                                 ...\n",
              "9965  How do you know you're girlfriend is getting t...\n",
              "9966  Kids telling dirty jokes http://www.vice.com/s...\n",
              "9967  How do we know that joan of arc was french ? s...\n",
              "9968  Ever heard of the 68 position? you go down on ...\n",
              "9969  I accidentally pooped my pants in the elevator...\n",
              "\n",
              "[9970 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-82bee4e7-da75-4a5b-a84f-2a7aa40b0f55\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What do you call a turtle without its shell? d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is a pokemon master's favorite kind of pa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why do native americans hate it when it rains ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My family tree is a cactus, we're all pricks.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How are music and candy similar? we throw away...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9965</th>\n",
              "      <td>How do you know you're girlfriend is getting t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9966</th>\n",
              "      <td>Kids telling dirty jokes http://www.vice.com/s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9967</th>\n",
              "      <td>How do we know that joan of arc was french ? s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9968</th>\n",
              "      <td>Ever heard of the 68 position? you go down on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9969</th>\n",
              "      <td>I accidentally pooped my pants in the elevator...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9970 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82bee4e7-da75-4a5b-a84f-2a7aa40b0f55')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-82bee4e7-da75-4a5b-a84f-2a7aa40b0f55 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-82bee4e7-da75-4a5b-a84f-2a7aa40b0f55');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning GPT2"
      ],
      "metadata": {
        "id": "t1udipMaL7Dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time torch is used because imo it's easier to fine-tune model with this framework\n"
      ],
      "metadata": {
        "id": "Yij4VQuxLhyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Jokes(Dataset):  \n",
        "    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=32):\n",
        "\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
        "        self.jokes = []\n",
        "\n",
        "        for row in jokes_df['text']:\n",
        "          self.jokes.append(torch.tensor(\n",
        "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
        "            ))               \n",
        "        if truncate:\n",
        "            self.jokes = self.jokes[:20000]\n",
        "        self.count = len(self.jokes)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.count\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.jokes[item]\n",
        "    \n",
        "dataset = Jokes(jokes_df['text'].values, truncate=True, gpt2_type=\"gpt2\")  "
      ],
      "metadata": {
        "id": "BrMVv3mIv_DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "#Accumulated batch size (since GPT2 is so big)\n",
        "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
        "    if packed_tensor is None:\n",
        "        return new_tensor, True, None\n",
        "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
        "        return packed_tensor, False, new_tensor\n",
        "    else:\n",
        "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
        "        return packed_tensor, True, None"
      ],
      "metadata": {
        "id": "vt_Th58Kwe_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    dataset, model,\n",
        "    batch_size=16, epochs=5, lr=2e-5,\n",
        "    max_seq_len=40, warmup_steps=200,\n",
        "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
        "    test_mode=False,save_model_on_epoch=False,\n",
        "):\n",
        "    acc_steps = 100\n",
        "    device=torch.device(\"cuda\")\n",
        "    model = model.cuda()\n",
        "    model.train()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "    loss=0\n",
        "    accumulating_batch_count = 0\n",
        "    input_tensor = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        print(f\"Training epoch {epoch}\")\n",
        "        print(loss)\n",
        "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
        "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
        "\n",
        "            if carry_on and idx != len(train_dataloader) - 1:\n",
        "                continue\n",
        "\n",
        "            input_tensor = input_tensor.to(device)\n",
        "            outputs = model(input_tensor, labels=input_tensor)\n",
        "            loss = outputs[0]\n",
        "            loss.backward()\n",
        "\n",
        "            if (accumulating_batch_count % batch_size) == 0:\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "\n",
        "            accumulating_batch_count += 1\n",
        "            input_tensor = None\n",
        "        if save_model_on_epoch:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
        "            )\n",
        "    return model"
      ],
      "metadata": {
        "id": "dhUKjRCwwkKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train(dataset, model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IUEHdHpwoKY",
        "outputId": "08e524e8-08cf-48a5-ed47-ccb6589ac73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 0\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9970it [06:02, 27.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 1\n",
            "tensor(0.7847, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9970it [06:00, 27.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 2\n",
            "tensor(0.3385, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9970it [06:00, 27.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 3\n",
            "tensor(0.3183, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9970it [06:00, 27.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 4\n",
            "tensor(0.2852, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9970it [06:00, 27.62it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    entry_count=10,\n",
        "    entry_length=30, #maximum number of words\n",
        "    top_p=0.8,\n",
        "    temperature=1.,\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "\n",
        "    filter_value = -float(\"Inf\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in trange(entry_count):\n",
        "\n",
        "            entry_finished = False\n",
        "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "\n",
        "            for i in range(entry_length):\n",
        "                outputs = model(generated, labels=generated)\n",
        "                loss, logits = outputs[:2]\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                    ..., :-1\n",
        "                ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "\n",
        "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "                generated = torch.cat((generated, next_token), dim=1)\n",
        "\n",
        "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
        "                    entry_finished = True\n",
        "\n",
        "                if entry_finished:\n",
        "\n",
        "                    generated_num = generated_num + 1\n",
        "\n",
        "                    output_list = list(generated.squeeze().numpy())\n",
        "                    output_text = tokenizer.decode(output_list)\n",
        "                    generated_list.append(output_text)\n",
        "                    break\n",
        "            \n",
        "            if not entry_finished:\n",
        "              output_list = list(generated.squeeze().numpy())\n",
        "              output_text = f\"{tokenizer.decode(output_list)}\" \n",
        "              generated_list.append(output_text)\n",
        "                \n",
        "    return generated_list\n"
      ],
      "metadata": {
        "id": "XMofQihD04c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_generation(test_data):\n",
        "  generated_joke = []\n",
        "  for i in range(len(test_data)):\n",
        "    x = generate(model.to('cpu'), tokenizer, test_set['text'][i], entry_count=1)\n",
        "    generated_joke.append(x)\n",
        "  return generated_joke"
      ],
      "metadata": {
        "id": "6ASUh6XD07M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_joke = text_generation(test_set['text'].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-Hvpspq1GK-",
        "outputId": "7b1a3ae2-237a-4975-edff-9294533c1bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.73s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.24s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.46s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.23s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.57s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.16s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.33s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.28s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.35s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.30s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.30s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.16s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.35s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.53s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.08s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.31s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.19s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.81s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.26s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.31s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.56s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.20s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.48s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.20s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.90s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RESULTS"
      ],
      "metadata": {
        "id": "IdSE4IoOL1Fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for joke in generated_joke:\n",
        "  print(joke[0])\n",
        "  print('________\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIcTe6Gh9CQ-",
        "outputId": "c6717c9e-c05e-4bb9-9a5c-16879fc1522f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How can a cat walk on the moon? A search of the Google  search engine revealed nothing. It may even go without saying that the answer is still in the question\n",
            "________\n",
            "\n",
            "So a frog parked his rifle in the middle of the road\n",
            "\n",
            "A young man walked in\n",
            "\n",
            "He didn't know what to say\n",
            "\n",
            "He just ran out\n",
            "\n",
            "________\n",
            "\n",
            "What did the baby seal say? What did the tusk say?<|endoftext|>\n",
            "________\n",
            "\n",
            "What do you with 365 days to think? Thinking!<|endoftext|>\n",
            "________\n",
            "\n",
            "That allah guy sure is a smart guy and that's why she wants to marry him. She'll go down on him if she gets pregnant.\"\n",
            "\n",
            "\"What's your\n",
            "________\n",
            "\n",
            "What do you call two jobs that take two days to earn a living? a laborer's day job. A laborer's overtime is the same as a week of work\n",
            "________\n",
            "\n",
            "Honey the baby is crowning!\n",
            "  I know my baby isn't going to show\n",
            "  why he can't be.  I'll teach you to ask him\n",
            "________\n",
            "\n",
            "Top 10 ways to avoid dating.\n",
            "\n",
            "#10: Avoid spending the night with your mother.\n",
            "\n",
            "I know you're already spending the night at home but I'll\n",
            "________\n",
            "\n",
            "They say a moose can cross the street without even a dog in sight.<|endoftext|>\n",
            "________\n",
            "\n",
            "Keep your friend's clothes in the kitchen to make yourself stand out.\n",
            "In the morning you'll be told that you have to do it, that you're going to owe someone\n",
            "________\n",
            "\n",
            "What is a chiropractor's favorite game to play, and why do you do it? Because you love it!\n",
            "\n",
            "Do you like? Play with it!\n",
            "\n",
            "Do you\n",
            "________\n",
            "\n",
            "My gpa is underwater i'm falling and i'm bleeding. i'm like an hour away from home.<|endoftext|>\n",
            "________\n",
            "\n",
            "If we were compressed down into four different genres, we would have many different results. This is why you should always be concerned about the compression ratio of the songs on your computer\n",
            "________\n",
            "\n",
            "A giraffe walks into a crowd.\n",
            "\n",
            "...\n",
            "\n",
            "\"You know what I think? I'm going to have to go play fetch with my mate and the rest of\n",
            "________\n",
            "\n",
            "What do you call black people when they get angry? dead.\n",
            " \"I'm a black man. I'm fired.\" \"I'm a thug.\" \"I'm fired\n",
            "________\n",
            "\n",
            "My friend woke up this morning. I looked at my phone, and my sister was sitting there. 'What's wrong?' I asked. 'I didn't hear you coming\n",
            "________\n",
            "\n",
            "Im not a morning person.\n",
            " \"I just want to be awake.\"\n",
            " \"What do you call a morning person without a job? dead.\"\n",
            "That's what it's\n",
            "________\n",
            "\n",
            "There are 10 kinds of blasters on your gear.\n",
            "\n",
            "\n",
            "1. Proximity Mine\n",
            "\n",
            "Any bolt of wind you can shoot into the open can easily be knocked\n",
            "________\n",
            "\n",
            "Does your wife have dry shampoo on?\n",
            "\n",
            "Can she thicken her hair?\n",
            "\n",
            "How do you know that there is no special bathroom in your neighborhood that doesn't\n",
            "________\n",
            "\n",
            "Dwarf shortage i know, it's not as bad as you think but i'll be paying one or two more cents and this is the day. this place is in a spanish ghetto\n",
            "________\n",
            "\n",
            "What did the baby corn say when she was in diapers? \"Honey, I'm taking this time to reflect on my mum's new baby.\"\n",
            "How do we know\n",
            "________\n",
            "\n",
            "Why did lot leave his mother's husband behind?\n",
            "Maiden was out of state. She had been playing around with dogs in the community. She was bored with walking.\n",
            "________\n",
            "\n",
            "Today i found out what is happening to artists like nana, and i took my money<|endoftext|>\n",
            "________\n",
            "\n",
            "What state do the most commonsense pieces of legislation have in common? Whose position is best?\n",
            "\n",
            "Schaefer\n",
            "\n",
            "Your seat is so blue. If you\n",
            "________\n",
            "\n",
            "Dishes are like boyfriends. my boyfriend gets me a pair of boxer shorts and a pair of socks. he is my girlfriend and she is my boyfriend.\n",
            "\n",
            "\n",
            "\n",
            "Once he comes\n",
            "________\n",
            "\n",
            "What do you call the s-word, because it's the?\"\n",
            "\"That's right, it's the s.\"\n",
            "\"Why do you call it that?\"\n",
            "________\n",
            "\n",
            "I'm so old i thought i was asleep and i'm scared i'm going to die. now i'm wearing white, I'm walking down the street. i'm in an\n",
            "________\n",
            "\n",
            "Where can you always find a store with ebay for all of your electronics? It's available in most locales and it only sells in certain locations, but when you go\n",
            "________\n",
            "\n",
            "What is a pirate's favourite kind of pasta? Try! Spaghetti! Spaghetti...<|endoftext|>\n",
            "________\n",
            "\n",
            "Past, present, and future walk and swim the well-worn path to your destination.\n",
            "\n",
            "\n",
            "Advanced<|endoftext|>\n",
            "________\n",
            "\n"
          ]
        }
      ]
    }
  ]
}